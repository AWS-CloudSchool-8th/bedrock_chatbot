from chains.qa_chain import build_qa_chain
from retrievers.kb_retriever import get_kb_retriever, get_llm

# ê²€ìƒ‰ score ê¸°ì¤€ (ì´í•˜ì¼ ê²½ìš° ì‹¤íŒ¨ë¡œ ê°„ì£¼)
RELEVANCE_THRESHOLD = 0.5

def answer_question(question: str):
    retriever = get_kb_retriever()
    llm = get_llm()

    docs = retriever.invoke(question)

    # Bedrockì—ì„œ ë°˜í™˜í•œ score í™•ì¸
    high_quality_docs = [
        doc for doc in docs
        if doc.metadata.get("score", 1.0) >= RELEVANCE_THRESHOLD
    ]

    if high_quality_docs:
        print("ğŸ“š KB ê²€ìƒ‰ ì„±ê³µ â†’ QA ì²´ì¸ ì‹¤í–‰")
        qa_chain = build_qa_chain()
        response = qa_chain.invoke(question)

        # invoke ê²°ê³¼ëŠ” ë”•ì…”ë„ˆë¦¬ -> ë©”íƒ€ ë°ì´í„°ë„ ê°™ì´ ì¶œë ¥
        # answer = response["result"] if isinstance(response, dict) else response

         # QA ì²´ì¸ì€ dict í˜•íƒœë¡œ ë°˜í™˜ë¨ â†’ resultë§Œ ì¶”ì¶œ
        answer = response["result"] if isinstance(response, dict) else str(response)
    else:
        print("ğŸŒ KB ê²€ìƒ‰ ì‹¤íŒ¨ â†’ Claude ë‹¨ë… ì‘ë‹µ")
        # ë©”íƒ€ ë°ì´í„°ë„ ê°™ì´ ì¶œë ¥
        # answer = llm.invoke(question)

        response = llm.invoke(question)

        # Claude ì‘ë‹µ ê°ì²´ì—ì„œ contentë§Œ ì¶”ì¶œ
        answer = response.content if hasattr(response, "content") else str(response)
        # âœ… ì—¬ê¸°ì„œ ê¸¸ì´ ì œí•œ ì²˜ë¦¬!
        # if len(answer) > 20:
        #     answer = answer[:20] + "..."

    return answer